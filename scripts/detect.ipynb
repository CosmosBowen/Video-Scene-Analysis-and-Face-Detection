{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **IMPORT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import face_recognition\n",
    "from PIL import Image, ImageDraw\n",
    "from IPython.display import display  \n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import os\n",
    "import json\n",
    "\n",
    "from scenedetect import detect, AdaptiveDetector, split_video_ffmpeg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **DETECTION FUNCTIONS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_resize_image(pil_image, size=3):\n",
    "    primary_dimention = pil_image.size.index(max(pil_image.size))\n",
    "    secondary_dimention = int(1 - primary_dimention)\n",
    "\n",
    "    primary_size = size * 100\n",
    "    ratio = primary_size / float(pil_image.size[primary_dimention])\n",
    "    secondary_size = int(float(pil_image.size[secondary_dimention]) * ratio)\n",
    "\n",
    "    width = primary_size if primary_dimention == 0 else secondary_size\n",
    "    height = secondary_size if primary_dimention == 0 else primary_size\n",
    "    resized_image = pil_image.resize((width, height), Image.Resampling.LANCZOS)\n",
    "    display(resized_image)\n",
    "\n",
    "def get_bbox(locations):\n",
    "    bbox = []\n",
    "    for location in locations:\n",
    "        top, right, bottom, left = location\n",
    "        x = left\n",
    "        y = top\n",
    "        width = right - left\n",
    "        height = bottom - top\n",
    "        face_coords = [x, y, width, height]\n",
    "        bbox.append(face_coords)\n",
    "\n",
    "    return bbox\n",
    "\n",
    "def get_bbox_area(face_location):\n",
    "    top, right, bottom, left = face_location\n",
    "    width = right - left\n",
    "    height = bottom - top\n",
    "    area = width * height\n",
    "    return area\n",
    "\n",
    "def get_marked_pil(frame_rgb, results, locations):\n",
    "    pil_marked = Image.fromarray(frame_rgb)\n",
    "    draw = ImageDraw.Draw(pil_marked)\n",
    "    for res, location in zip(results, locations):\n",
    "        top, right, bottom, left = location\n",
    "        color = \"red\" if res else \"black\"\n",
    "        draw.rectangle(\n",
    "            ((left, top), (right, bottom)),\n",
    "            outline=color,\n",
    "            width=20\n",
    "        )\n",
    "    return pil_marked\n",
    "\n",
    "def sort_locations_with_desc_area(area_locations_list): # area_locations format: (area, locations)\n",
    "    sorted_area_locations = sorted(area_locations_list, key=lambda x: x[0], reverse=True)\n",
    "    sorted_locations = [area_locations[1] for area_locations in sorted_area_locations]\n",
    "    return sorted_locations\n",
    "\n",
    "def convert_relative_path_to_absolute(path): # relative path can be either file or folder\n",
    "    return f'project/{str(Path(path).resolve()).split(\"project/\")[-1]}'\n",
    "\n",
    "\n",
    "def visualize_detect_result(face_encodings, target_encoding, results, locations, position_name, frame_rgb, found):\n",
    "    num_detected_faces = len(np.where(results)[0])\n",
    "    if found:\n",
    "        message = f\"found target face\"\n",
    "        emoji = \"✅\"\n",
    "    else:\n",
    "        message = \"no match target face, but found other faces\"\n",
    "        emoji = \"❌\"\n",
    "    \n",
    "    distances = face_recognition.face_distance(face_encodings, target_encoding)\n",
    "    print(f\"> {emoji} Checked {position_name} frame ({message}, {num_detected_faces}/{len(face_encodings)})\")\n",
    "    print(\"distances:\")\n",
    "    for distance in sorted(distances):\n",
    "        print(f'- {distance}')\n",
    "\n",
    "    pil_marked = get_marked_pil(frame_rgb, results, locations)\n",
    "    display_resize_image(pil_marked, size=3)\n",
    "        \n",
    "def get_visible_face_locations(face_locations, image_area, desc_sorted_by_area=True):\n",
    "    locations_list = []\n",
    "\n",
    "    for face_location in face_locations:\n",
    "        bbox_area = get_bbox_area(face_location)\n",
    "        ratio = (bbox_area / image_area) * 100\n",
    "        if ratio > 1:\n",
    "            if desc_sorted_by_area:\n",
    "                locations_list.append((bbox_area, face_location))\n",
    "            else:\n",
    "                locations_list.append(face_location)\n",
    "\n",
    "        return sort_locations_with_desc_area(locations_list) if desc_sorted_by_area else locations_list  \n",
    "\n",
    "def get_sorted_face_locations(face_locations):\n",
    "    area_locations_list = []\n",
    "    for face_location in face_locations:\n",
    "        bbox_area = get_bbox_area(face_location)\n",
    "        area_locations_list.append((bbox_area, face_location))\n",
    "\n",
    "    sorted_locations = sort_locations_with_desc_area(area_locations_list)\n",
    "    return sorted_locations\n",
    "\n",
    "def find_face_locations(image_path, only_visible=False, show_progress=False, border_color=\"blue\", desc_sorted_by_area = False, show_image_size=3):\n",
    "    final_face_locations = None\n",
    "\n",
    "    pil_image = Image.open(image_path)\n",
    "    faces = face_recognition.load_image_file(image_path)\n",
    "    face_locations = face_recognition.face_locations(faces)\n",
    "\n",
    "    if len(face_locations) == 0:\n",
    "        print(\"no face detected\")\n",
    "        return None\n",
    "    \n",
    "    if only_visible:\n",
    "        image_area = pil_image.size[0] * pil_image.size[1]\n",
    "        final_face_locations = get_visible_face_locations(face_locations, image_area, desc_sorted_by_area)\n",
    "        if show_progress:\n",
    "            print(f\"detected {len(final_face_locations)}/{len(face_locations)} VISIBLE faces.\")\n",
    "    else:\n",
    "        final_face_locations = get_sorted_face_locations(face_locations) if desc_sorted_by_area else face_locations\n",
    "        if show_progress:\n",
    "            print(f\"detected {len(final_face_locations)} faces\")\n",
    "\n",
    "    if show_progress:\n",
    "        draw = ImageDraw.Draw(pil_image)\n",
    "        for i, location in enumerate(final_face_locations):\n",
    "            top, right, bottom, left = location\n",
    "            if desc_sorted_by_area:\n",
    "                color = \"red\" if i == 0 else border_color\n",
    "            else:\n",
    "                color = border_color\n",
    "            draw.rectangle(\n",
    "                ((left, top), (right, bottom)),\n",
    "                outline=color,\n",
    "                width=20\n",
    "            )\n",
    "\n",
    "        display_resize_image(pil_image, size=show_image_size)\n",
    "\n",
    "    return final_face_locations\n",
    "\n",
    "def encode_target_face(image_path, only_visible=True):\n",
    "    target_face = face_recognition.load_image_file(image_path)\n",
    "    sorted_target_face_locations = find_face_locations(\n",
    "        image_path, \n",
    "        only_visible, \n",
    "        show_progress=True, \n",
    "        border_color=\"black\",\n",
    "        desc_sorted_by_area=True,\n",
    "    )\n",
    "\n",
    "    if sorted_target_face_locations is None:\n",
    "        display_resize_image(Image.open(image_path), size=2)\n",
    "        raise Exception(f\"No face detected, ensure to have human face in your reference image {image_path}\")\n",
    "    \n",
    "    face_location_with_largest_area = sorted_target_face_locations[0]\n",
    "    target_encoding = face_recognition.face_encodings(target_face, [face_location_with_largest_area])[0]\n",
    "    return target_encoding\n",
    "    \n",
    "def detect_and_match_faces(frame, target_encoding, tolerance):\n",
    "    # Detect all faces in frame\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) \n",
    "    locations = face_recognition.face_locations(frame_rgb)\n",
    "    if len(locations) == 0: # no face\n",
    "        return None\n",
    "    \n",
    "    # Get face encodings\n",
    "    face_encodings = face_recognition.face_encodings(frame_rgb, locations)\n",
    "\n",
    "    # Compare with target face\n",
    "    results = face_recognition.compare_faces(face_encodings, target_encoding, tolerance)\n",
    "    detected_face_indices = np.where(results)[0]\n",
    "    num_detected_faces = len(detected_face_indices)\n",
    "    if num_detected_faces == 0: # has face, but no match\n",
    "        return None\n",
    "    \n",
    "    # Get bbox for all detected target faces\n",
    "    detected_face_locations = [locations[i] for i in detected_face_indices]\n",
    "    bbox = get_bbox(detected_face_locations)\n",
    "\n",
    "    # Mark all faces\n",
    "    pil_marked = get_marked_pil(frame_rgb, results, locations)\n",
    "    marked_frame = cv2.cvtColor(np.array(pil_marked), cv2.COLOR_RGB2BGR) # Convert PIL image back to BGR for video writing\n",
    "    \n",
    "    # Return coordinates of matched face\n",
    "    return num_detected_faces, bbox, marked_frame\n",
    "\n",
    "def save_metadata(metadata, file_path):\n",
    "    with open(file_path, \"w\") as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    print(f\"saved metadata to {convert_relative_path_to_absolute(file_path)}\")\n",
    "\n",
    "\n",
    "class FaceDetector:\n",
    "\n",
    "    def __init__(self, video_name_with_extension, reference_image_name_with_extension, tolerance):\n",
    "        self.VIDEO_PATH = f\"../source/{video_name_with_extension}\"\n",
    "        self.IMAGE_PATH = f\"../source/{reference_image_name_with_extension}\"\n",
    "        \n",
    "        self.VIDEO_NAME = video_name_with_extension\n",
    "        self.VIDEO_NAME_ONLY = os.path.splitext(video_name_with_extension)[0]\n",
    "        \n",
    "        self.IMAGE_NAME = reference_image_name_with_extension\n",
    "        self.IMAGE_NAME_ONLY = os.path.splitext(reference_image_name_with_extension)[0]\n",
    "\n",
    "        self.TOLERANCE = tolerance\n",
    "\n",
    "        self.NUM_SCENES = None\n",
    "        self.FPS = None\n",
    "        self.WIDTH = None\n",
    "        self.HEIGHT = None\n",
    "\n",
    "        self.TARGET_ENCODING = None\n",
    "\n",
    "        self.METADATA = None\n",
    "\n",
    "        self.SCENES_INFO_START_END_TIME = []\n",
    "\n",
    "        self.FULL_CLIP = None\n",
    "        self.FULL_CLIP_MARKED = None\n",
    "\n",
    "        self.FOUND_MATCHED = False\n",
    "\n",
    "    def run(self, show_process=False):\n",
    "        print(\"\\n-------- Encoding Target Face --------\")\n",
    "        self.TARGET_ENCODING = encode_target_face(self.IMAGE_PATH, only_visible=True)\n",
    "        self.access_video()\n",
    "        self.create_folders()\n",
    "        self.initiate_metadata()\n",
    "        print(\"\\n-------- Splitting Video Into Scenes --------\")\n",
    "        scene_list = self.split_video_into_scenes()\n",
    "        self.cut_video_into_scenes(scene_list)\n",
    "        self.ready_full_clips()\n",
    "        print(\"\\n-------- Processing Scenes --------\")\n",
    "        self.process_scenes(show_process)\n",
    "        print(\"\\n-------- Saving Metadata --------\")\n",
    "        save_metadata(self.METADATA, self.METADATA_PATH)\n",
    "        self.finalize_full_clips()\n",
    "\n",
    "    def create_folders(self): # if not exist yet\n",
    "        results_dir = f\"../results/{self.IMAGE_NAME_ONLY}_{self.VIDEO_NAME_ONLY}\"\n",
    "        self.DITECT_SCENES_DIR = f\"{results_dir}/detect_scenes\"\n",
    "        cropped_videos_dir = f\"{results_dir}/cropped_videos\"\n",
    "        self.NORMAL_CLIPS_DIR = f\"{cropped_videos_dir}/normal\"\n",
    "        self.MARKED_CLIPS_DIR = f\"{cropped_videos_dir}/marked\"\n",
    "\n",
    "        self.FULL_CLIP_PATH = f\"{results_dir}/full_clip.mp4\"\n",
    "        self.FULL_CLIP_MARKED_PATH = f\"{results_dir}/full_clip_marked.mp4\"\n",
    "\n",
    "        self.METADATA_PATH = f\"{results_dir}/metadata.json\"\n",
    "\n",
    "        folders = [\n",
    "            self.DITECT_SCENES_DIR, \n",
    "            self.NORMAL_CLIPS_DIR, \n",
    "            self.MARKED_CLIPS_DIR\n",
    "        ]\n",
    "        for dir in folders:\n",
    "            os.makedirs(dir, exist_ok=True)\n",
    "\n",
    "    def access_video(self):\n",
    "        cap = cv2.VideoCapture(self.VIDEO_PATH)\n",
    "        if not cap.isOpened():\n",
    "            raise Exception(f\"Could not open video file {self.VIDEO_PATH}\")\n",
    "            \n",
    "        # Get video properties\n",
    "        self.FPS = cap.get(cv2.CAP_PROP_FPS)\n",
    "        self.WIDTH = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        self.HEIGHT = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        cap.release()\n",
    "    \n",
    "    def initiate_metadata(self):\n",
    "        self.METADATA = {\n",
    "            \"clips\": [],\n",
    "            \"video_properties\": {\n",
    "                \"fps\": self.FPS,\n",
    "                \"width\": self.WIDTH,\n",
    "                \"height\": self.HEIGHT\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def split_video_into_scenes(self):\n",
    "        scene_list = detect(self.VIDEO_PATH, AdaptiveDetector())\n",
    "        self.NUM_SCENES = len(scene_list)\n",
    "        if self.NUM_SCENES == 0:\n",
    "            raise Exception(f\"Progress stopped, because no scene can be split from video {self.VIDEO_NAME}\")\n",
    "        \n",
    "        self.FPS = scene_list[0][0].framerate\n",
    "\n",
    "        for i in range(self.NUM_SCENES):\n",
    "            scene = scene_list[i]\n",
    "            start_timecode = str(scene[0]) # Format: HH:MM:SS.nnn\n",
    "            end_timecode = str(scene[1])  \n",
    "            self.SCENES_INFO_START_END_TIME.append([start_timecode, end_timecode])\n",
    "\n",
    "        print(f\"detected {self.NUM_SCENES} scenes from video {self.VIDEO_NAME_ONLY},\")\n",
    "        return scene_list\n",
    "    \n",
    "    def cut_video_into_scenes(self, scene_list):\n",
    "        def custom_scene_formatter(scene, video):\n",
    "            return f\"scene_{scene.index + 1}.mp4\" # Format: scene_1.mp4\n",
    "\n",
    "        split_video_ffmpeg(\n",
    "            self.VIDEO_PATH,\n",
    "            scene_list,\n",
    "            output_dir=self.DITECT_SCENES_DIR,\n",
    "            formatter=custom_scene_formatter,\n",
    "            show_progress=True\n",
    "        )\n",
    "\n",
    "        print(f\"saved in {convert_relative_path_to_absolute(self.DITECT_SCENES_DIR)}\")\n",
    "\n",
    "    def ready_full_clips(self):\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        self.FULL_CLIP = cv2.VideoWriter(self.FULL_CLIP_PATH, fourcc, self.FPS, (self.WIDTH, self.HEIGHT))\n",
    "        self.FULL_CLIP_MARKED = cv2.VideoWriter(self.FULL_CLIP_MARKED_PATH, fourcc, self.FPS, (self.WIDTH, self.HEIGHT))\n",
    "    \n",
    "    def finalize_full_clips(self):\n",
    "        self.FULL_CLIP.release()\n",
    "        self.FULL_CLIP_MARKED.release()\n",
    "\n",
    "        if not self.FOUND_MATCHED:\n",
    "            os.remove(self.FULL_CLIP_PATH)\n",
    "            os.remove(self.FULL_CLIP_MARKED_PATH)\n",
    "            print(f\"Not found any target face at all throughout the entire video {convert_relative_path_to_absolute(self.VIDEO_PATH)}\")\n",
    "        else:\n",
    "            print(f\"Full clip saved to: {convert_relative_path_to_absolute(self.FULL_CLIP_PATH)}\")\n",
    "            print(f\"Marked full clip saved to: {convert_relative_path_to_absolute(self.FULL_CLIP_MARKED_PATH)}\")\n",
    "            \n",
    "\n",
    "    def process_scenes(self, show_process=False):\n",
    "        for i in range(self.NUM_SCENES):\n",
    "            scene_id = i + 1\n",
    "            clip_info = self.process_scene(scene_id, show_process)\n",
    "            if clip_info is not None:\n",
    "                self.METADATA[\"clips\"].append(clip_info)\n",
    "                if not self.FOUND_MATCHED:\n",
    "                    self.FOUND_MATCHED = True\n",
    "            \n",
    "            if not show_process:\n",
    "                message = \"✅ target face detected\" if clip_info is not None else \"❌ no match\"\n",
    "                print(f\"processed scene {scene_id}: {message}\")\n",
    "        \n",
    "        print(\"finished processing all scenes.\")\n",
    "\n",
    "    def process_scene(self, scene_id, show_process=False):\n",
    "        scene_path = f'{self.DITECT_SCENES_DIR}/scene_{scene_id}.mp4'\n",
    "        scene_cap = cv2.VideoCapture(scene_path)\n",
    "\n",
    "        if not scene_cap.isOpened():\n",
    "            print(f\"Could not open scene video {scene_path}, skip it.\")\n",
    "            return None\n",
    "\n",
    "        total_frames = int(scene_cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "        if show_process:\n",
    "            print(f'\\n**** Processing Scene: {scene_id} ****')\n",
    "\n",
    "        # Check if scene samples has target face\n",
    "        matched_face = self.check_scene(scene_cap, total_frames, show_process)\n",
    "        if not matched_face:\n",
    "            if show_process:\n",
    "                print(\"no need to further detect all frames\")\n",
    "            return None\n",
    "        \n",
    "        # Get to process all frames\n",
    "        if show_process:\n",
    "            print(f'> now processing all frames')\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # or 'XVID'\n",
    "        clip_path = f\"{self.NORMAL_CLIPS_DIR}/scene_{scene_id}.mp4\"\n",
    "        clip = cv2.VideoWriter(clip_path, fourcc, self.FPS, (self.WIDTH, self.HEIGHT))\n",
    "        \n",
    "        marked_clip_path = f\"{self.MARKED_CLIPS_DIR}/scene_{scene_id}.mp4\"\n",
    "        marked_clip = cv2.VideoWriter(marked_clip_path, fourcc, self.FPS, (self.WIDTH, self.HEIGHT))\n",
    "\n",
    "        clip, marked_clip, clip_info_target_face_coordinates = self.process_all_frames(scene_cap, total_frames, clip, marked_clip, show_process)\n",
    "        \n",
    "        # Release video writer\n",
    "        clip.release()\n",
    "        marked_clip.release()\n",
    "\n",
    "        if show_process:\n",
    "            print(f\"> Clip saved to: {convert_relative_path_to_absolute(clip_path)}\")\n",
    "            print(f\"> Marked clip saved to: {convert_relative_path_to_absolute(marked_clip_path)}\")\n",
    "        \n",
    "        start_timecode, end_timecode = self.SCENES_INFO_START_END_TIME[scene_id - 1]\n",
    "        clip_info = {\n",
    "            \"file_name\": f\"scene_{scene_id}.mp4\",\n",
    "            \"start_timecode\": start_timecode,\n",
    "            \"end_timecode\": end_timecode,\n",
    "            \"target_face_coordinates\": clip_info_target_face_coordinates\n",
    "        }\n",
    "        return clip_info\n",
    "    \n",
    "    def check_scene(self, scene_cap, total_frames, show_process=False):\n",
    "        \"\"\"\n",
    "        Check first, middle and last frames from a scene video for target face\n",
    "        \"\"\"\n",
    "\n",
    "        # Define frame positions to check\n",
    "        positions = {\n",
    "            'first': 0,\n",
    "            'middle': total_frames // 2,\n",
    "            'last': total_frames - 1\n",
    "        }\n",
    "\n",
    "        for position_name, frame_idx in positions.items():\n",
    "            # Set frame position\n",
    "            scene_cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
    "            ret, frame = scene_cap.read()\n",
    "            \n",
    "            if ret:\n",
    "                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) # Convert frame to RGB and to PIL Image\n",
    "                locations = face_recognition.face_locations(frame_rgb)\n",
    "                if len(locations) == 0: # no face\n",
    "                    if show_process:\n",
    "                        print(f\"> 🫥 No face in {position_name} frame\")\n",
    "                    continue\n",
    "                face_encodings = face_recognition.face_encodings(frame_rgb, locations)\n",
    "                results = face_recognition.compare_faces(face_encodings, self.TARGET_ENCODING, self.TOLERANCE)\n",
    "                if len(np.where(results)[0]) == 0: # has faces, but no match\n",
    "                    if show_process:\n",
    "                        visualize_detect_result(face_encodings, self.TARGET_ENCODING, results, locations, position_name, frame_rgb, found=False)\n",
    "                    continue\n",
    "                else:\n",
    "                    if show_process:\n",
    "                        visualize_detect_result(face_encodings, self.TARGET_ENCODING, results, locations, position_name, frame_rgb, found=True)\n",
    "                    return True\n",
    "\n",
    "        return False\n",
    "    \n",
    "    def process_all_frames(self, scene_cap, total_frames, clip, marked_clip, show_process=False):\n",
    "        clip_info_target_face_coordinates = []\n",
    "        frame_number = 0\n",
    "        \n",
    "        # Reset video capture to beginning\n",
    "        scene_cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "\n",
    "        while True:\n",
    "            ret, frame = scene_cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            frame_number += 1\n",
    "            results = detect_and_match_faces(frame, self.TARGET_ENCODING, self.TOLERANCE)\n",
    "            if results is None: # this frame no detect target face, go to next frame\n",
    "                continue\n",
    "            \n",
    "            num_detected_faces, bbox, marked_frame = results\n",
    "            clip_info_target_face_coordinates.append(\n",
    "                {\n",
    "                    \"frame_id\": frame_number,\n",
    "                    \"num_detected_faces\": num_detected_faces,\n",
    "                    \"bbox\": bbox\n",
    "                }\n",
    "            )\n",
    "\n",
    "            clip.write(frame)\n",
    "            marked_clip.write(marked_frame)\n",
    "\n",
    "            self.FULL_CLIP.write(frame)\n",
    "            self.FULL_CLIP_MARKED.write(marked_frame)\n",
    "\n",
    "\n",
    "            if show_process:\n",
    "                if frame_number % 10 == 0:  # Print progress every 10 frames\n",
    "                    print(f\"> Checked frames: {frame_number}/{total_frames}\")\n",
    "\n",
    "        if show_process:\n",
    "            print(\"> Done!\")\n",
    "            print(f\"> Number of Clip frames VS Original Scene frames: {len(clip_info_target_face_coordinates)}/{total_frames}\")\n",
    "        \n",
    "        return clip, marked_clip, clip_info_target_face_coordinates\n",
    "\n",
    "\n",
    "def run_detect(reference_face_name, video_name, tolerance, show_process):\n",
    "    face_detector = FaceDetector(\n",
    "        video_name, \n",
    "        reference_face_name, \n",
    "        tolerance\n",
    "    )\n",
    "    face_detector.run(show_process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **RUN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_detect(\n",
    "    reference_face_name = \"reference_face.jpg\", \n",
    "    video_name = \"video.mp4\", \n",
    "    tolerance = 0.679, \n",
    "    show_process = False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "face_detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
